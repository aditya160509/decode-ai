[
  {
    "model_name": "GPT 5 (fast)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 256000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.00125,
    "output_price_per_1k_tokens_usd": 0.01,
    "average_latency_seconds": 0.38,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "real time chat basic retrieval",
    "availability_or_plan": "API only",
    "compute_hours_per_1m_tokens": 0.7,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "burst caps per minute"
  },
  {
    "model_name": "GPT 5 (medium)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 256000,
    "max_output_tokens": 16384,
    "input_price_per_1k_tokens_usd": 0.0015,
    "output_price_per_1k_tokens_usd": 0.011,
    "average_latency_seconds": 0.7,
    "typical_reasoning_depth_supported": "chain of thought three to five steps",
    "memory_or_session_support": "full",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and some image",
    "rated_tasks": "multi turn QA coding",
    "availability_or_plan": "API and Pro",
    "compute_hours_per_1m_tokens": 1.2,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "minute and hour caps"
  },
  {
    "model_name": "GPT 5 (high)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 256000,
    "max_output_tokens": 32768,
    "input_price_per_1k_tokens_usd": 0.0025,
    "output_price_per_1k_tokens_usd": 0.014,
    "average_latency_seconds": 1.5,
    "typical_reasoning_depth_supported": "long context multi step planning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text image experimental audio",
    "rated_tasks": "advanced reasoning research codegen",
    "availability_or_plan": "API and Enterprise",
    "compute_hours_per_1m_tokens": 2.0,
    "token_compression_efficiency": 1.3,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "daily request caps"
  },
  {
    "model_name": "Claude 4.5 Opus",
    "provider": "Anthropic",
    "release_or_current_version": "4.5 2025",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 32768,
    "input_price_per_1k_tokens_usd": 0.015,
    "output_price_per_1k_tokens_usd": 0.075,
    "average_latency_seconds": 1.2,
    "typical_reasoning_depth_supported": "long context multi step planning agentic",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "deep coding agents research",
    "availability_or_plan": "API paid",
    "compute_hours_per_1m_tokens": 1.8,
    "token_compression_efficiency": 1.25,
    "scaling_penalty_for_parallel_sessions": "medium high",
    "notes_on_provider_limits": "concurrency limits per tier"
  },
  {
    "model_name": "Claude 4.5 Sonnet",
    "provider": "Anthropic",
    "release_or_current_version": "4.5 2025",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 64000,
    "input_price_per_1k_tokens_usd": 0.003,
    "output_price_per_1k_tokens_usd": 0.015,
    "average_latency_seconds": 0.8,
    "typical_reasoning_depth_supported": "multi step planning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "coding content chat",
    "availability_or_plan": "API free tier available",
    "compute_hours_per_1m_tokens": 1.1,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "good throughput"
  },
  {
    "model_name": "Gemini 2.5 Pro",
    "provider": "Google DeepMind",
    "release_or_current_version": "2.5 Pro 2025",
    "max_context_window_tokens": 1048576,
    "max_output_tokens": 65536,
    "input_price_per_1k_tokens_usd": 0.0075,
    "output_price_per_1k_tokens_usd": 0.03,
    "average_latency_seconds": 1.4,
    "typical_reasoning_depth_supported": "long context planning thinking",
    "memory_or_session_support": "yes thinking tokens",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text image video audio",
    "rated_tasks": "advanced reasoning video analysis",
    "availability_or_plan": "Vertex AI and Google AI Studio",
    "compute_hours_per_1m_tokens": 2.1,
    "token_compression_efficiency": 1.4,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "thinking tokens priced as output"
  },
  {
    "model_name": "Gemini 2.5 Flash",
    "provider": "Google DeepMind",
    "release_or_current_version": "2.5 Flash 2025",
    "max_context_window_tokens": 1048576,
    "max_output_tokens": 65536,
    "input_price_per_1k_tokens_usd": 0.00075,
    "output_price_per_1k_tokens_usd": 0.003,
    "average_latency_seconds": 0.5,
    "typical_reasoning_depth_supported": "chain of thought with thinking",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text image video audio",
    "rated_tasks": "high volume processing fast retrieval",
    "availability_or_plan": "Vertex AI and Google AI",
    "compute_hours_per_1m_tokens": 0.85,
    "token_compression_efficiency": 1.35,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "audio input priced higher"
  },
  {
    "model_name": "DeepSeek R1 0528",
    "provider": "DeepSeek",
    "release_or_current_version": "R1 0528 2025",
    "max_context_window_tokens": 65536,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.00055,
    "output_price_per_1k_tokens_usd": 0.0022,
    "average_latency_seconds": 3.5,
    "typical_reasoning_depth_supported": "deep reasoning long thinking",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "complex math coding logic",
    "availability_or_plan": "API and open weights",
    "compute_hours_per_1m_tokens": 4.5,
    "token_compression_efficiency": 1.3,
    "scaling_penalty_for_parallel_sessions": "high",
    "notes_on_provider_limits": "long thinking overhead"
  },
  {
    "model_name": "DeepSeek V3.1",
    "provider": "DeepSeek",
    "release_or_current_version": "V3.1 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.00027,
    "output_price_per_1k_tokens_usd": 0.0011,
    "average_latency_seconds": 0.9,
    "typical_reasoning_depth_supported": "multi step",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "chat coding analysis",
    "availability_or_plan": "API and chat",
    "compute_hours_per_1m_tokens": 1.0,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "very low price"
  },
  {
    "model_name": "Llama 3 70B",
    "provider": "Meta",
    "release_or_current_version": "Llama 3 2024",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.00035,
    "output_price_per_1k_tokens_usd": 0.00105,
    "average_latency_seconds": 0.18,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "general chat reasoning coding",
    "availability_or_plan": "API and open source",
    "compute_hours_per_1m_tokens": 0.65,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "provider dependent"
  },
  {
    "model_name": "Mixtral 8x7B",
    "provider": "Mistral",
    "release_or_current_version": "Mixtral 2023",
    "max_context_window_tokens": 32768,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.00035,
    "output_price_per_1k_tokens_usd": 0.00035,
    "average_latency_seconds": 0.22,
    "typical_reasoning_depth_supported": "multi step",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "chat coding reasoning",
    "availability_or_plan": "API and open source",
    "compute_hours_per_1m_tokens": 0.4,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "MoE efficient"
  },
  {
    "model_name": "Codestral",
    "provider": "Mistral",
    "release_or_current_version": "Codestral 25.08",
    "max_context_window_tokens": 256000,
    "max_output_tokens": 16384,
    "input_price_per_1k_tokens_usd": 0.0003,
    "output_price_per_1k_tokens_usd": 0.0009,
    "average_latency_seconds": 0.25,
    "typical_reasoning_depth_supported": "code multi step planning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and code",
    "rated_tasks": "code generation refactor",
    "availability_or_plan": "API and open weights",
    "compute_hours_per_1m_tokens": 0.5,
    "token_compression_efficiency": 1.3,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "fast and efficient"
  },
  {
    "model_name": "Command R Plus",
    "provider": "Cohere",
    "release_or_current_version": "Command R Plus 1.5",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.0025,
    "output_price_per_1k_tokens_usd": 0.01,
    "average_latency_seconds": 0.75,
    "typical_reasoning_depth_supported": "RAG optimized",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "RAG agents tool use",
    "availability_or_plan": "API paid",
    "compute_hours_per_1m_tokens": 1.1,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "improved throughput"
  },
  {
    "model_name": "GPT 4.1",
    "provider": "OpenAI",
    "release_or_current_version": "4.1 2024",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.01,
    "output_price_per_1k_tokens_usd": 0.03,
    "average_latency_seconds": 0.6,
    "typical_reasoning_depth_supported": "short chain of thought",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "chat QA research",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 1.2,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "minute caps"
  },
  {
    "model_name": "GPT 5 (low)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.001,
    "output_price_per_1k_tokens_usd": 0.008,
    "average_latency_seconds": 0.22,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "no",
    "multimodal_support": "text only",
    "rated_tasks": "simple chat basic agents",
    "availability_or_plan": "API only",
    "compute_hours_per_1m_tokens": 0.5,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "limited persistent context"
  },
  {
    "model_name": "GPT 5 (nano)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 16384,
    "max_output_tokens": 2048,
    "input_price_per_1k_tokens_usd": 0.0005,
    "output_price_per_1k_tokens_usd": 0.004,
    "average_latency_seconds": 0.07,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "no",
    "multimodal_support": "text only",
    "rated_tasks": "education light chat",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.2,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "context resets often"
  },
  {
    "model_name": "GPT 5 (mini)",
    "provider": "OpenAI",
    "release_or_current_version": "v5 2025",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 1024,
    "input_price_per_1k_tokens_usd": 0.0003,
    "output_price_per_1k_tokens_usd": 0.003,
    "average_latency_seconds": 0.05,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "no",
    "multimodal_support": "text only",
    "rated_tasks": "short answers small bots",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.1,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "batch friendly"
  },
  {
    "model_name": "GPT 5 Codex",
    "provider": "OpenAI",
    "release_or_current_version": "v5 codex 2025",
    "max_context_window_tokens": 32768,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.003,
    "output_price_per_1k_tokens_usd": 0.016,
    "average_latency_seconds": 1.0,
    "typical_reasoning_depth_supported": "tool and multi step planning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and code",
    "rated_tasks": "programming codegen",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 2.5,
    "token_compression_efficiency": 1.3,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "code exec rate caps"
  },
  {
    "model_name": "GPT 4 Turbo",
    "provider": "OpenAI",
    "release_or_current_version": "4 turbo 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 12288,
    "input_price_per_1k_tokens_usd": 0.006,
    "output_price_per_1k_tokens_usd": 0.018,
    "average_latency_seconds": 0.38,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "interactive chat code search",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.95,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "limited max context"
  },
  {
    "model_name": "O3",
    "provider": "OpenAI",
    "release_or_current_version": "o3 2025",
    "max_context_window_tokens": 64000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.008,
    "output_price_per_1k_tokens_usd": 0.024,
    "average_latency_seconds": 0.9,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "qa planning light code",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 1.1,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "medium",
    "notes_on_provider_limits": "short context cap"
  },
  {
    "model_name": "O3 Pro",
    "provider": "OpenAI",
    "release_or_current_version": "o3 pro 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 12288,
    "input_price_per_1k_tokens_usd": 0.012,
    "output_price_per_1k_tokens_usd": 0.036,
    "average_latency_seconds": 1.3,
    "typical_reasoning_depth_supported": "multi step",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "advanced qa",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 1.4,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "medium high",
    "notes_on_provider_limits": "expensive and slower"
  },
  {
    "model_name": "GPT 3.5 Turbo",
    "provider": "OpenAI",
    "release_or_current_version": "3.5 turbo 2023",
    "max_context_window_tokens": 16384,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.0005,
    "output_price_per_1k_tokens_usd": 0.0015,
    "average_latency_seconds": 0.08,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "no",
    "multimodal_support": "text only",
    "rated_tasks": "basic chat simple queries",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.13,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "context resets quickly"
  },
  {
    "model_name": "Claude 4.5 Haiku",
    "provider": "Anthropic",
    "release_or_current_version": "4.5 haiku 2025",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 64000,
    "input_price_per_1k_tokens_usd": 0.001,
    "output_price_per_1k_tokens_usd": 0.005,
    "average_latency_seconds": 0.15,
    "typical_reasoning_depth_supported": "basic to chain of thought",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "fast chat customer service",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.3,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "batch discount available"
  },
  {
    "model_name": "Claude 4.1 Opus",
    "provider": "Anthropic",
    "release_or_current_version": "4.1 opus 2024",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 32768,
    "input_price_per_1k_tokens_usd": 0.015,
    "output_price_per_1k_tokens_usd": 0.075,
    "average_latency_seconds": 1.3,
    "typical_reasoning_depth_supported": "long context multi step reasoning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "complex reasoning coding",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 1.9,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "medium high",
    "notes_on_provider_limits": "premium pricing"
  },
  {
    "model_name": "Claude 4.1 Sonnet",
    "provider": "Anthropic",
    "release_or_current_version": "4.1 sonnet 2024",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 64000,
    "input_price_per_1k_tokens_usd": 0.003,
    "output_price_per_1k_tokens_usd": 0.015,
    "average_latency_seconds": 0.85,
    "typical_reasoning_depth_supported": "multi step planning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "general coding content",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 1.15,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "standard limits"
  },
  {
    "model_name": "Claude 4.1 Haiku",
    "provider": "Anthropic",
    "release_or_current_version": "4.1 haiku 2024",
    "max_context_window_tokens": 200000,
    "max_output_tokens": 64000,
    "input_price_per_1k_tokens_usd": 0.0008,
    "output_price_per_1k_tokens_usd": 0.0024,
    "average_latency_seconds": 0.18,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "lightweight chat classification",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.32,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "budget friendly"
  },
  {
    "model_name": "Gemini Nano",
    "provider": "Google DeepMind",
    "release_or_current_version": "nano 2024",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.000075,
    "output_price_per_1k_tokens_usd": 0.00015,
    "average_latency_seconds": 0.05,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "no",
    "multimodal_support": "text only",
    "rated_tasks": "on device lightweight",
    "availability_or_plan": "on device",
    "compute_hours_per_1m_tokens": 0.1,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "not applicable",
    "notes_on_provider_limits": "offline"
  },
  {
    "model_name": "PaLM 2 Bison",
    "provider": "Google DeepMind",
    "release_or_current_version": "palm2 bison 2023",
    "max_context_window_tokens": 12800,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.0005,
    "output_price_per_1k_tokens_usd": 0.0015,
    "average_latency_seconds": 0.35,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "limited",
    "multimodal_support": "text only",
    "rated_tasks": "general text",
    "availability_or_plan": "vertex ai legacy",
    "compute_hours_per_1m_tokens": 0.5,
    "token_compression_efficiency": 1.05,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "deprecated prefer gemini"
  },
  {
    "model_name": "Grok 4",
    "provider": "xAI",
    "release_or_current_version": "grok4 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 16384,
    "input_price_per_1k_tokens_usd": 0.01,
    "output_price_per_1k_tokens_usd": 0.03,
    "average_latency_seconds": 1.8,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "reasoning search",
    "availability_or_plan": "xai api",
    "compute_hours_per_1m_tokens": 2.0,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "medium high",
    "notes_on_provider_limits": "rate limits per tier"
  },
  {
    "model_name": "Grok 4 fast reasoning",
    "provider": "xAI",
    "release_or_current_version": "grok4 fast 2025",
    "max_context_window_tokens": 2000000,
    "max_output_tokens": 32768,
    "input_price_per_1k_tokens_usd": 0.0002,
    "output_price_per_1k_tokens_usd": 0.0005,
    "average_latency_seconds": 2.55,
    "typical_reasoning_depth_supported": "efficient reasoning",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text image voice",
    "rated_tasks": "cost efficient reasoning and agents",
    "availability_or_plan": "xai api",
    "compute_hours_per_1m_tokens": 1.1,
    "token_compression_efficiency": 1.25,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "cached input cheaper"
  },
  {
    "model_name": "Grok code fast 1",
    "provider": "xAI",
    "release_or_current_version": "grok code fast 1 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.0002,
    "output_price_per_1k_tokens_usd": 0.0005,
    "average_latency_seconds": 0.6,
    "typical_reasoning_depth_supported": "fast code reasoning",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "fast coding assistance",
    "availability_or_plan": "xai api",
    "compute_hours_per_1m_tokens": 0.6,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "no published limits"
  },
  {
    "model_name": "DeepSeek Coder V2",
    "provider": "DeepSeek",
    "release_or_current_version": "coder v2 2025",
    "max_context_window_tokens": 16384,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.00014,
    "output_price_per_1k_tokens_usd": 0.00028,
    "average_latency_seconds": 0.4,
    "typical_reasoning_depth_supported": "code specific reasoning",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "code generation debugging",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.35,
    "token_compression_efficiency": 1.2,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "very low cost"
  },
  {
    "model_name": "Llama 3 8B",
    "provider": "Meta",
    "release_or_current_version": "llama3 8b 2024",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 2048,
    "input_price_per_1k_tokens_usd": 0.0001,
    "output_price_per_1k_tokens_usd": 0.0003,
    "average_latency_seconds": 0.04,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "limited",
    "multimodal_support": "text only",
    "rated_tasks": "lightweight chat edge",
    "availability_or_plan": "open source",
    "compute_hours_per_1m_tokens": 0.15,
    "token_compression_efficiency": 1.05,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "runs on consumer gpu"
  },
  {
    "model_name": "Code Llama 70B",
    "provider": "Meta",
    "release_or_current_version": "code llama 70b 2024",
    "max_context_window_tokens": 16384,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.0005,
    "output_price_per_1k_tokens_usd": 0.0015,
    "average_latency_seconds": 0.35,
    "typical_reasoning_depth_supported": "code specific multi step",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "code generation explanation",
    "availability_or_plan": "open source",
    "compute_hours_per_1m_tokens": 0.8,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "needs enterprise gpu"
  },
  {
    "model_name": "Mistral 7B v0.3",
    "provider": "Mistral",
    "release_or_current_version": "mistral 7b v0.3 2024",
    "max_context_window_tokens": 32768,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.0002,
    "output_price_per_1k_tokens_usd": 0.0002,
    "average_latency_seconds": 0.08,
    "typical_reasoning_depth_supported": "basic to chain of thought",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "light chat edge models",
    "availability_or_plan": "API and open source",
    "compute_hours_per_1m_tokens": 0.15,
    "token_compression_efficiency": 1.05,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "cost effective"
  },
  {
    "model_name": "Command R",
    "provider": "Cohere",
    "release_or_current_version": "command r 1.5 2025",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 8192,
    "input_price_per_1k_tokens_usd": 0.00015,
    "output_price_per_1k_tokens_usd": 0.0006,
    "average_latency_seconds": 0.45,
    "typical_reasoning_depth_supported": "rag and multi step",
    "memory_or_session_support": "yes",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text only",
    "rated_tasks": "rag simpler agents retrieval",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.6,
    "token_compression_efficiency": 1.15,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "budget option"
  },
  {
    "model_name": "Phi 3 Medium",
    "provider": "Microsoft",
    "release_or_current_version": "phi3 medium 2024",
    "max_context_window_tokens": 128000,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.0008,
    "output_price_per_1k_tokens_usd": 0.0024,
    "average_latency_seconds": 0.15,
    "typical_reasoning_depth_supported": "chain of thought",
    "memory_or_session_support": "partial",
    "tool_or_function_calling_support": "yes",
    "multimodal_support": "text and image",
    "rated_tasks": "lightweight reasoning multimodal",
    "availability_or_plan": "API",
    "compute_hours_per_1m_tokens": 0.3,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "azure native"
  },
  {
    "model_name": "Phi 3 Small",
    "provider": "Microsoft",
    "release_or_current_version": "phi3 small 2024",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 2048,
    "input_price_per_1k_tokens_usd": 0.0002,
    "output_price_per_1k_tokens_usd": 0.0006,
    "average_latency_seconds": 0.05,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "limited",
    "multimodal_support": "text only",
    "rated_tasks": "ultra lightweight",
    "availability_or_plan": "API and open source",
    "compute_hours_per_1m_tokens": 0.08,
    "token_compression_efficiency": 1.0,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "runs on consumer hardware"
  },
  {
    "model_name": "Gemma 7B",
    "provider": "Google",
    "release_or_current_version": "gemma 7b 2024",
    "max_context_window_tokens": 8192,
    "max_output_tokens": 4096,
    "input_price_per_1k_tokens_usd": 0.0001,
    "output_price_per_1k_tokens_usd": 0.0001,
    "average_latency_seconds": 0.003,
    "typical_reasoning_depth_supported": "basic single turn",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "limited",
    "multimodal_support": "text only",
    "rated_tasks": "light chat classification",
    "availability_or_plan": "open source",
    "compute_hours_per_1m_tokens": 0.12,
    "token_compression_efficiency": 1.05,
    "scaling_penalty_for_parallel_sessions": "low",
    "notes_on_provider_limits": "very fast on groq"
  },
  {
    "model_name": "Falcon 180B",
    "provider": "TII",
    "release_or_current_version": "falcon 180b 2023",
    "max_context_window_tokens": 4096,
    "max_output_tokens": 2048,
    "input_price_per_1k_tokens_usd": 0.001,
    "output_price_per_1k_tokens_usd": 0.003,
    "average_latency_seconds": 2.5,
    "typical_reasoning_depth_supported": "multi step reasoning",
    "memory_or_session_support": "no",
    "tool_or_function_calling_support": "limited",
    "multimodal_support": "text only",
    "rated_tasks": "large model reasoning",
    "availability_or_plan": "open source",
    "compute_hours_per_1m_tokens": 4.2,
    "token_compression_efficiency": 1.1,
    "scaling_penalty_for_parallel_sessions": "high",
    "notes_on_provider_limits": "needs multi gpu"
  }
]
